{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Install dependencies"
      ],
      "metadata": {
        "id": "_sepLyCfx96r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Grab ALL crypto data available and compress it to parquets in storage"
      ],
      "metadata": {
        "id": "fQpuq4-Wx1nS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ccxt yfinance pytrends tweepy praw newsapi-python aiohttp scipy wbdata tqdm pyarrow db-sqlite3 pyyaml tensorflow.keras"
      ],
      "metadata": {
        "id": "czItygEIY7xX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The project represents a sophisticated crypto data warehouse system with several key components that can be leveraged for advanced algorithmic trading. Let me break down how we can enhance this system using various machine learning approaches.\n",
        "\n",
        "Data Collection and Storage Architecture\n",
        "\n",
        "The current system efficiently collects data from multiple sources:\n",
        "\n",
        "Market data (CCXT, YFinance)\n",
        "Social sentiment (Twitter, Reddit)\n",
        "Technical indicators\n",
        "On-chain metrics\n",
        "\n",
        "The ParquetManager and MetadataManager provide a robust foundation for handling large-scale data storage. We can enhance this by implementing a feature store layer for machine learning:"
      ],
      "metadata": {
        "id": "tSQ7dUVjfqXB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Enhanced Feature Store Implementation"
      ],
      "metadata": {
        "id": "8PkZSQ4NgDV8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Dict, List, Optional\n",
        "import polars as pl\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "class MLFeatureStore:\n",
        "    def __init__(self, config):\n",
        "        self.parquet_manager = ParquetManager(config)\n",
        "        self.feature_pipeline = MemoryOptimizedFeaturePipeline(config)\n",
        "\n",
        "    async def create_training_dataset(\n",
        "        self,\n",
        "        lookback_window: int,\n",
        "        prediction_horizon: int,\n",
        "        feature_groups: List[str]\n",
        "    ) -> pl.DataFrame:\n",
        "        \"\"\"Create a dataset optimized for ML training.\"\"\"\n",
        "        features = []\n",
        "\n",
        "        # Market data features\n",
        "        if 'market' in feature_groups:\n",
        "            market_data = await self._load_market_features(lookback_window)\n",
        "            features.append(market_data)\n",
        "\n",
        "        # Sentiment features\n",
        "        if 'sentiment' in feature_groups:\n",
        "            sentiment_data = await self._load_sentiment_features(lookback_window)\n",
        "            features.append(sentiment_data)\n",
        "\n",
        "        # Technical indicators\n",
        "        if 'technical' in feature_groups:\n",
        "            technical_data = await self._compute_technical_features(lookback_window)\n",
        "            features.append(technical_data)\n",
        "\n",
        "        # Combine features and create labels\n",
        "        combined = pl.concat(features)\n",
        "        labels = self._generate_labels(combined, prediction_horizon)\n",
        "\n",
        "        return combined.join(labels, on='timestamp')\n",
        "\n",
        "    async def _load_market_features(self, lookback_window: int) -> pl.DataFrame:\n",
        "        \"\"\"Load and prepare market data features.\"\"\"\n",
        "        market_data = await self.parquet_manager.load_data(\n",
        "            self.config.data_dir / 'processed' / 'market_features.parquet'\n",
        "        )\n",
        "\n",
        "        # Create lagged features\n",
        "        for lag in range(1, lookback_window + 1):\n",
        "            market_data = market_data.with_columns([\n",
        "                pl.col('close').shift(lag).alias(f'close_lag_{lag}'),\n",
        "                pl.col('volume').shift(lag).alias(f'volume_lag_{lag}'),\n",
        "                pl.col('volatility').shift(lag).alias(f'volatility_lag_{lag}')\n",
        "            ])\n",
        "\n",
        "        return market_data\n",
        "\n",
        "    def _generate_labels(\n",
        "        self,\n",
        "        df: pl.DataFrame,\n",
        "        horizon: int\n",
        "    ) -> pl.DataFrame:\n",
        "        \"\"\"Generate multi-horizon prediction labels.\"\"\"\n",
        "        return df.select([\n",
        "            'timestamp',\n",
        "            (pl.col('close').shift(-horizon) / pl.col('close') - 1)\n",
        "                .alias(f'return_{horizon}'),\n",
        "            (pl.col('close').shift(-horizon) > pl.col('close'))\n",
        "                .alias(f'direction_{horizon}')\n",
        "        ])"
      ],
      "metadata": {
        "id": "tuMkbzJ9gDDy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------------------------------------------------\n",
        "# Grab Raw Data\n",
        "# --------------------------------------------------------------\n",
        "import os\n",
        "import pandas as pd\n",
        "import pyarrow.parquet as pq\n",
        "import pyarrow as pa\n",
        "import requests\n",
        "import ccxt\n",
        "import yfinance as yf\n",
        "import logging\n",
        "import time\n",
        "import random\n",
        "import concurrent.futures\n",
        "import tweepy\n",
        "import praw\n",
        "from newsapi import NewsApiClient\n",
        "import aiohttp\n",
        "import asyncio\n",
        "import wbdata  # World Bank Data API\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "# Define Google Drive Directory\n",
        "GOOGLE_DRIVE_DIR = \"/content/drive/MyDrive/data_warehouse\"\n",
        "\n",
        "# Set the threshold for old data (in days)\n",
        "DATA_EXPIRY_DAYS = 30\n",
        "\n",
        "# --------------------------------------------------------------\n",
        "# Logging Configuration\n",
        "# --------------------------------------------------------------\n",
        "logging.basicConfig(\n",
        "    filename=\"data_fetching.log\",\n",
        "    level=logging.INFO,\n",
        "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
        "    filemode='a'\n",
        ")\n",
        "\n",
        "# --------------------------------------------------------------\n",
        "# Google Drive Mounting\n",
        "# --------------------------------------------------------------\n",
        "def mount_google_drive(max_retries=5):\n",
        "    \"\"\"Mount Google Drive with a retry mechanism.\"\"\"\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            drive.mount('/content/drive', force_remount=True)\n",
        "            logging.info(\"Google Drive mounted successfully.\")\n",
        "            return True\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Drive mount attempt {attempt + 1} failed: {e}\")\n",
        "            if attempt < max_retries - 1:\n",
        "                time.sleep(2 ** attempt)\n",
        "            else:\n",
        "                logging.critical(\"Failed to mount Google Drive after several attempts.\")\n",
        "                return False\n",
        "\n",
        "# --------------------------------------------------------------\n",
        "# Utility Functions\n",
        "# --------------------------------------------------------------\n",
        "def retry_on_failure(max_retries=3, delay=2, backoff=2):\n",
        "    \"\"\"Decorator to retry a function on failure with exponential backoff.\"\"\"\n",
        "    def decorator(func):\n",
        "        def wrapper(*args, **kwargs):\n",
        "            for attempt in range(max_retries):\n",
        "                try:\n",
        "                    result = func(*args, **kwargs)\n",
        "                    logging.info(f\"Function {func.__name__} executed successfully on attempt {attempt + 1}.\")\n",
        "                    return result\n",
        "                except Exception as e:\n",
        "                    logging.error(f\"Attempt {attempt + 1} failed in function {func.__name__}: {e}\")\n",
        "                    if attempt < max_retries - 1:\n",
        "                        sleep_time = delay * (backoff ** attempt) + random.uniform(0, 1)\n",
        "                        logging.info(f\"Retrying {func.__name__} after {sleep_time:.2f} seconds...\")\n",
        "                        time.sleep(sleep_time)\n",
        "                    else:\n",
        "                        logging.critical(f\"Max retries reached. Function {func.__name__} failed.\")\n",
        "                        raise e\n",
        "        return wrapper\n",
        "    return decorator\n",
        "\n",
        "# --------------------------------------------------------------\n",
        "# Saving Data to Compressed Parquet with Snappy\n",
        "# --------------------------------------------------------------\n",
        "@retry_on_failure()\n",
        "def save_to_parquet(df, file_name, dir_path):\n",
        "    \"\"\"Save DataFrame to a compressed Parquet file with Snappy compression.\"\"\"\n",
        "    try:\n",
        "        if df.empty:\n",
        "            raise ValueError(f\"DataFrame is empty. Skipping file: {file_name}\")\n",
        "        if df.isnull().values.any():\n",
        "            raise ValueError(f\"DataFrame contains NaN values. Skipping file: {file_name}\")\n",
        "\n",
        "        file_path = os.path.join(dir_path, file_name)\n",
        "        os.makedirs(dir_path, exist_ok=True)  # Ensure directory exists\n",
        "        table = pa.Table.from_pandas(df)\n",
        "\n",
        "        # Save Parquet with Snappy compression\n",
        "        pq.write_table(table, file_path, compression='snappy')\n",
        "        logging.info(f\"Data saved to {file_path} with Snappy compression\")\n",
        "        print(f\"File successfully saved: {file_path}\")\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Failed to save data to {file_name}: {e}\")\n",
        "        raise e\n",
        "\n",
        "# --------------------------------------------------------------\n",
        "# API Initialization Functions (Twitter, Reddit, NewsAPI)\n",
        "# --------------------------------------------------------------\n",
        "def init_twitter_api():\n",
        "    api_key = \"SICXIAhJiu1EZs6x2LT4ZYhoY\"\n",
        "    api_secret_key = \"d1lbP2oNNvMMO5173KBhpOqRpgr8pH9wycUU0hA5oVTeqUKPWi\"\n",
        "    auth = tweepy.AppAuthHandler(api_key, api_secret_key)\n",
        "    api = tweepy.API(auth)\n",
        "    return api\n",
        "\n",
        "def init_reddit_api():\n",
        "    client_id = \"GusCH1g6lbueOv4cjUylfA\"\n",
        "    client_secret = \"Gwwl2xRAqz0FyG4XiQFPa5YzTIYcKw\"\n",
        "    user_agent = \"Sentiment\"\n",
        "    reddit = praw.Reddit(client_id=client_id, client_secret=client_secret, user_agent=user_agent)\n",
        "    return reddit\n",
        "\n",
        "# --------------------------------------------------------------\n",
        "# Data Fetching Functions (API-specific)\n",
        "# --------------------------------------------------------------\n",
        "@retry_on_failure()\n",
        "def fetch_twitter_data(keywords, max_tweets=100):\n",
        "    \"\"\"Fetch Twitter data for given keywords.\"\"\"\n",
        "    try:\n",
        "        api = init_twitter_api()\n",
        "        for keyword in keywords:\n",
        "            tweets = tweepy.Cursor(api.search_tweets, q=keyword, lang=\"en\").items(max_tweets)\n",
        "            tweet_data = [[tweet.created_at, tweet.text, tweet.user.screen_name] for tweet in tweets]\n",
        "            df = pd.DataFrame(tweet_data, columns=['created_at', 'text', 'user'])\n",
        "            file_name = f\"twitter_{keyword}.parquet\"\n",
        "            save_to_parquet(df, file_name, GOOGLE_DRIVE_DIR)\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Failed to fetch Twitter data for {keywords}: {e}\")\n",
        "\n",
        "@retry_on_failure()\n",
        "def fetch_reddit_data(subreddits, max_posts=100):\n",
        "    \"\"\"Fetch Reddit data from specified subreddits.\"\"\"\n",
        "    try:\n",
        "        reddit = init_reddit_api()\n",
        "        for subreddit_name in subreddits:\n",
        "            subreddit = reddit.subreddit(subreddit_name)\n",
        "            posts = subreddit.top(limit=max_posts)\n",
        "            post_data = [[post.created_utc, post.title, post.selftext, post.score] for post in posts]\n",
        "            df = pd.DataFrame(post_data, columns=['created_utc', 'title', 'selftext', 'score'])\n",
        "            df['created_utc'] = pd.to_datetime(df['created_utc'], unit='s')\n",
        "            file_name = f\"reddit_{subreddit_name}.parquet\"\n",
        "            save_to_parquet(df, file_name, GOOGLE_DRIVE_DIR)\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Failed to fetch Reddit data for {subreddits}: {e}\")\n",
        "\n",
        "@retry_on_failure()\n",
        "def fetch_alpha_vantage_data(symbol, interval='1min', outputsize='compact'):\n",
        "    \"\"\"Fetch financial data from Alpha Vantage for a given symbol.\"\"\"\n",
        "    try:\n",
        "        api_key = \"R95N2BARMVWY25VT\"\n",
        "        url = f\"https://www.alphavantage.co/query?function=TIME_SERIES_INTRADAY&symbol={symbol}&interval={interval}&outputsize={outputsize}&apikey={api_key}\"\n",
        "        response = requests.get(url)\n",
        "        data = response.json()\n",
        "        time_series = data.get(f'Time Series ({interval})', {})\n",
        "        df = pd.DataFrame(time_series).transpose().reset_index()\n",
        "        df.columns = ['timestamp', 'open', 'high', 'low', 'close', 'volume']\n",
        "        df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
        "        file_name = f\"alpha_vantage_{symbol}_{interval}.parquet\"\n",
        "        save_to_parquet(df, file_name, GOOGLE_DRIVE_DIR)\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Failed to fetch Alpha Vantage data for {symbol}: {e}\")\n",
        "\n",
        "@retry_on_failure()\n",
        "def fetch_world_bank_data(indicator, country='all', start_date='2000', end_date='2023'):\n",
        "    \"\"\"Fetch global development data from the World Bank API.\"\"\"\n",
        "    try:\n",
        "        df = wbdata.get_dataframe({indicator: indicator}, country=country, data_date=(start_date, end_date))\n",
        "        df.reset_index(inplace=True)\n",
        "        file_name = f\"world_bank_{indicator}.parquet\"\n",
        "        save_to_parquet(df, file_name, GOOGLE_DRIVE_DIR)\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Failed to fetch World Bank data for {indicator}: {e}\")\n",
        "\n",
        "@retry_on_failure()\n",
        "def fetch_imf_data(database_id, indicator, country='all', start_year='2000', end_year='2023'):\n",
        "    \"\"\"Fetch global economic data from the IMF API.\"\"\"\n",
        "    try:\n",
        "        url = f\"http://www.bd-econ.com/imfapi1.html?db={database_id}&series={indicator}&start_year={start_year}&end_year={end_year}&country={country}\"\n",
        "        response = requests.get(url)\n",
        "        data = response.json()\n",
        "        df = pd.DataFrame(data['CompactData']['DataSet']['Series']['Obs'])\n",
        "        file_name = f\"imf_{indicator}.parquet\"\n",
        "        save_to_parquet(df, file_name, GOOGLE_DRIVE_DIR)\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Failed to fetch IMF data for {indicator}: {e}\")\n",
        "\n",
        "# --------------------------------------------------------------\n",
        "# Asynchronous Fetching for CoinMarketCap and CoinGecko\n",
        "# --------------------------------------------------------------\n",
        "async def fetch_async(url, headers=None, params=None):\n",
        "    \"\"\"Asynchronous HTTP requests for data fetching.\"\"\"\n",
        "    async with aiohttp.ClientSession() as session:\n",
        "        async with session.get(url, headers=headers, params=params) as response:\n",
        "            if response.status == 200:\n",
        "                return await response.json()\n",
        "            else:\n",
        "                print(f\"Failed to fetch data: HTTP {response.status}\")\n",
        "                return None\n",
        "\n",
        "async def fetch_and_save_async(url, headers, params, file_name, dir_path):\n",
        "    \"\"\"Asynchronous fetch and save data to Parquet.\"\"\"\n",
        "    data = await fetch_async(url, headers=headers, params=params)\n",
        "    if data:\n",
        "        df = pd.DataFrame(data['data'])\n",
        "        save_to_parquet(df, file_name, dir_path)\n",
        "\n",
        "async def fetch_coinmarketcap_data_async():\n",
        "    url = \"https://pro-api.coinmarketcap.com/v1/cryptocurrency/listings/latest\"\n",
        "    headers = {\n",
        "        'Accepts': 'application/json',\n",
        "        'X-CMC_PRO_API_KEY': 'your_api_key_here',  # Replace with your actual API key\n",
        "    }\n",
        "    params = {\n",
        "        'start': '1',\n",
        "        'limit': '250',\n",
        "        'convert': 'USD'\n",
        "    }\n",
        "    file_name = \"coinmarketcap_data.parquet\"\n",
        "    await fetch_and_save_async(url, headers, params, file_name, GOOGLE_DRIVE_DIR)\n",
        "\n",
        "async def fetch_coingecko_data_async():\n",
        "    url = \"https://api.coingecko.com/api/v3/coins/markets\"\n",
        "    params = {\n",
        "        'vs_currency': 'usd',\n",
        "        'order': 'market_cap_desc',\n",
        "        'per_page': '250',\n",
        "        'page': '1',\n",
        "        'sparkline': 'false'\n",
        "    }\n",
        "    file_name = \"coingecko_data.parquet\"\n",
        "    await fetch_and_save_async(url, None, params, file_name, GOOGLE_DRIVE_DIR)\n",
        "\n",
        "# --------------------------------------------------------------\n",
        "# CCXT Data Fetching with Subfolder Management\n",
        "# --------------------------------------------------------------\n",
        "@retry_on_failure()\n",
        "def fetch_data_for_exchange(exchange_id, timeframes=['1m', '5m', '15m', '1h', '1d'], limit=1000, max_failures=10):\n",
        "    \"\"\"Fetch OHLCV data for all markets on a specified exchange using CCXT.\"\"\"\n",
        "    try:\n",
        "        exchange = getattr(ccxt, exchange_id)()\n",
        "        markets = exchange.load_markets()\n",
        "        failure_count = 0\n",
        "\n",
        "        for symbol in markets:\n",
        "            if failure_count >= max_failures:\n",
        "                logging.warning(f\"Reached {max_failures} consecutive failures for {exchange_id}. Moving to next exchange.\")\n",
        "                break\n",
        "\n",
        "            for timeframe in timeframes:\n",
        "                if exchange.has['fetchOHLCV']:\n",
        "                    try:\n",
        "                        dir_path = os.path.join(GOOGLE_DRIVE_DIR, \"CCXT\", exchange_id, symbol.replace(\"/\", \"_\"), timeframe)\n",
        "                        file_name = f\"{exchange_id}_{symbol.replace('/', '_')}_{timeframe}.parquet\"\n",
        "                        if os.path.exists(os.path.join(dir_path, file_name)):\n",
        "                            continue  # Skip if file already exists\n",
        "                        ohlcv = exchange.fetch_ohlcv(symbol, timeframe, limit=limit)\n",
        "                        df = pd.DataFrame(ohlcv, columns=['timestamp', 'open', 'high', 'low', 'close', 'volume'])\n",
        "                        df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms')\n",
        "                        save_to_parquet(df, file_name, dir_path)\n",
        "                        failure_count = 0\n",
        "                    except Exception as e:\n",
        "                        logging.error(f\"Failed to fetch data from {exchange_id} for {symbol} with {timeframe} timeframe.\")\n",
        "                        failure_count += 1\n",
        "                        if failure_count >= max_failures:\n",
        "                            logging.warning(f\"Skipping {exchange_id} after {failure_count} consecutive failures.\")\n",
        "                            break\n",
        "\n",
        "        time.sleep(1)\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Failed to load markets for {exchange_id}: {e}\")\n",
        "\n",
        "def fetch_all_ccxt_data_parallel(timeframes=['1m', '5m', '15m', '1h', '1d'], limit=1000, max_failures=10, max_workers=10):\n",
        "    \"\"\"Fetch data from all exchanges in parallel using CCXT.\"\"\"\n",
        "    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
        "        futures = []\n",
        "        for exchange_id in ccxt.exchanges:\n",
        "            futures.append(executor.submit(fetch_data_for_exchange, exchange_id, timeframes, limit, max_failures))\n",
        "        for future in concurrent.futures.as_completed(futures):\n",
        "            try:\n",
        "                future.result()\n",
        "            except Exception as e:\n",
        "                logging.error(f\"An error occurred during parallel execution: {e}\")\n",
        "\n",
        "# --------------------------------------------------------------\n",
        "# YFinance Data Fetching (Dynamic Tickers)\n",
        "# --------------------------------------------------------------\n",
        "@retry_on_failure()\n",
        "def fetch_yfinance_data():\n",
        "    \"\"\"Fetch financial data from Yahoo Finance with dynamic tickers and subfoldering.\"\"\"\n",
        "    asset_types = {\n",
        "        'indices': '^GSPC ^IXIC ^DJI ^RUT ^VIX ^FTSE ^N225 ^HSI ^SSEC ^DAX ^FCHI ^BSESN ^STI ^AORD ^KS11 ^TWII ^GSPTSE'.split(),\n",
        "        'commodities': 'GC=F SI=F CL=F NG=F HG=F PL=F PA=F ZW=F ZC=F ZS=F LE=F HE=F'.split(),\n",
        "        'forex': 'EURUSD=X JPY=X GBPUSD=X AUDUSD=X NZDUSD=X USDCHF=X USDCAD=X'.split(),\n",
        "        'bonds': 'TNX TLT BND'.split(),\n",
        "        'mutual_funds': 'VFINX VTSMX FBGRX'.split(),\n",
        "        'etfs': 'SPY QQQ DIA GLD VXX'.split(),\n",
        "        'stocks': 'AAPL MSFT GOOGL AMZN TSLA FB NVDA JPM V MA DIS HD'.split()\n",
        "    }\n",
        "\n",
        "    periods = [\"1mo\", \"3mo\", \"6mo\", \"1y\", \"5y\", \"max\"]\n",
        "    intervals = [\"1m\", \"5m\", \"15m\", \"1h\", \"1d\", \"1wk\", \"1mo\"]\n",
        "\n",
        "    for asset_type, symbols in asset_types.items():\n",
        "        for symbol in symbols:\n",
        "            for period in periods:\n",
        "                for interval in intervals:\n",
        "                    try:\n",
        "                        file_name = f\"{asset_type}_{symbol}_{period}_{interval}.parquet\"\n",
        "                        file_path = os.path.join(GOOGLE_DRIVE_DIR, \"YFinance\", asset_type, symbol)\n",
        "                        if os.path.exists(file_path):\n",
        "                            continue\n",
        "                        data = yf.download(symbol, period=period, interval=interval)\n",
        "                        if not data.empty:\n",
        "                            data.reset_index(inplace=True)\n",
        "                            save_to_parquet(data, file_name, file_path)\n",
        "                    except Exception as e:\n",
        "                        logging.error(f\"Failed to fetch data for {symbol} ({asset_type}) with period {period} and interval {interval}.\")\n",
        "\n",
        "# --------------------------------------------------------------\n",
        "# Main Execution\n",
        "# --------------------------------------------------------------\n",
        "if mount_google_drive():\n",
        "    # Fetch CCXT data in parallel\n",
        "    fetch_all_ccxt_data_parallel()\n",
        "\n",
        "    # Fetch Yahoo Finance data dynamically with subfoldering\n",
        "    fetch_yfinance_data()\n",
        "\n",
        "    # Fetch Twitter, Reddit, etc.\n",
        "    fetch_twitter_data(keywords=['Bitcoin', 'Cryptocurrency', 'BTC'])\n",
        "    fetch_reddit_data(subreddits=['Bitcoin', 'CryptoCurrency', 'Crypto', 'BTC'])\n",
        "\n",
        "    # Fetch from Alpha Vantage, World Bank, IMF, EIA, Fear & Greed, CryptoPanic\n",
        "    fetch_alpha_vantage_data(symbol='BTCUSD')\n",
        "    fetch_world_bank_data(indicator='NY.GDP.MKTP.CD')\n",
        "    fetch_imf_data(database_id='WEO', indicator='NGDP_RPCH')\n",
        "\n",
        "    # Asynchronous CoinMarketCap and CoinGecko data fetching\n",
        "    #buggdincolab:) asyncio.run(fetch_coinmarketcap_data_async())\n",
        "    #buggdincolab:) asyncio.run(fetch_coingecko_data_async())\n"
      ],
      "metadata": {
        "id": "MGcwyiiLrWvz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Machine Learning Integration\n",
        "\n",
        "We can enhance the existing LSTM implementation by creating a multi-model ensemble system that leverages different architectures:"
      ],
      "metadata": {
        "id": "Cfr_ouxSgQCh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from typing import Dict, List\n",
        "import tensorflow as tf\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "import xgboost as xgb\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import torch\n",
        "\n",
        "class EnsemblePredictor:\n",
        "    def __init__(self, config):\n",
        "        self.config = config\n",
        "        self.models = {}\n",
        "        self.weights = {}\n",
        "\n",
        "    async def initialize_models(self):\n",
        "        \"\"\"Initialize all models in the ensemble.\"\"\"\n",
        "        # LSTM for time series\n",
        "        self.models['lstm'] = self._create_lstm_model()\n",
        "\n",
        "        # Transformer for sequence modeling\n",
        "        self.models['transformer'] = self._create_transformer_model()\n",
        "\n",
        "        # XGBoost for tabular data\n",
        "        self.models['xgboost'] = xgb.XGBRegressor(\n",
        "            objective='reg:squarederror',\n",
        "            tree_method='gpu_hist',\n",
        "            max_depth=6,\n",
        "            learning_rate=0.1\n",
        "        )\n",
        "\n",
        "        # Random Forest for robust predictions\n",
        "        self.models['rf'] = RandomForestClassifier(\n",
        "            n_estimators=100,\n",
        "            max_depth=10,\n",
        "            n_jobs=-1\n",
        "        )\n",
        "\n",
        "        # Initialize weights equally\n",
        "        model_count = len(self.models)\n",
        "        self.weights = {name: 1.0/model_count for name in self.models}\n",
        "\n",
        "    def _create_lstm_model(self) -> tf.keras.Model:\n",
        "        \"\"\"Create LSTM model with attention.\"\"\"\n",
        "        return tf.keras.Sequential([\n",
        "            tf.keras.layers.LSTM(128, return_sequences=True),\n",
        "            tf.keras.layers.MultiHeadAttention(\n",
        "                num_heads=4, key_dim=32\n",
        "            ),\n",
        "            tf.keras.layers.GlobalAveragePooling1D(),\n",
        "            tf.keras.layers.Dense(64, activation='relu'),\n",
        "            tf.keras.layers.Dense(1)\n",
        "        ])\n",
        "\n",
        "    def _create_transformer_model(self) -> torch.nn.Module:\n",
        "        \"\"\"Create a custom transformer for time series.\"\"\"\n",
        "        config = self.config.model.transformer\n",
        "        return AutoModelForSequenceClassification.from_pretrained(\n",
        "            config.base_model,\n",
        "            num_labels=1\n",
        "        )\n",
        "\n",
        "    async def predict(self, features: Dict[str, np.ndarray]) -> np.ndarray:\n",
        "        \"\"\"Generate ensemble predictions.\"\"\"\n",
        "        predictions = {}\n",
        "\n",
        "        # Get predictions from each model\n",
        "        for model_name, model in self.models.items():\n",
        "            if model_name == 'lstm':\n",
        "                pred = model.predict(features['sequence_data'])\n",
        "            elif model_name == 'transformer':\n",
        "                pred = self._get_transformer_predictions(\n",
        "                    features['sequence_data']\n",
        "                )\n",
        "            else:  # XGBoost and RF\n",
        "                pred = model.predict(features['tabular_data'])\n",
        "            predictions[model_name] = pred\n",
        "\n",
        "        # Weighted ensemble\n",
        "        final_prediction = sum(\n",
        "            self.weights[name] * pred\n",
        "            for name, pred in predictions.items()\n",
        "        )\n",
        "\n",
        "        return final_prediction\n",
        "\n",
        "    async def update_weights(\n",
        "        self,\n",
        "        performance_metrics: Dict[str, float]\n",
        "    ):\n",
        "        \"\"\"Update model weights based on performance.\"\"\"\n",
        "        total_score = sum(performance_metrics.values())\n",
        "        self.weights = {\n",
        "            name: score/total_score\n",
        "            for name, score in performance_metrics.items()\n",
        "        }"
      ],
      "metadata": {
        "id": "5U7ZwFdvgSAo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature Engineering and Processing\n",
        "\n",
        "The existing FeaturePipeline can be enhanced to support more advanced features:"
      ],
      "metadata": {
        "id": "WLYOP4WOgZxZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import polars as pl\n",
        "from typing import List, Dict\n",
        "import numpy as np\n",
        "from scipy import stats\n",
        "import talib\n",
        "\n",
        "class AdvancedFeatureEngineer:\n",
        "    def __init__(self, config):\n",
        "        self.config = config\n",
        "\n",
        "    def compute_advanced_features(self, df: pl.DataFrame) -> pl.DataFrame:\n",
        "        \"\"\"Compute advanced trading features.\"\"\"\n",
        "        expressions = []\n",
        "\n",
        "        # Volatility features\n",
        "        expressions.extend(self._volatility_features())\n",
        "\n",
        "        # Market regime features\n",
        "        expressions.extend(self._market_regime_features())\n",
        "\n",
        "        # Order book features\n",
        "        expressions.extend(self._orderbook_features())\n",
        "\n",
        "        # Cross-asset correlation features\n",
        "        expressions.extend(self._correlation_features())\n",
        "\n",
        "        return df.with_columns(expressions)\n",
        "\n",
        "    def _volatility_features(self) -> List[pl.Expr]:\n",
        "        \"\"\"Complex volatility indicators.\"\"\"\n",
        "        return [\n",
        "            # Realized volatility\n",
        "            (pl.col('returns').rolling_std(window=20) * np.sqrt(252))\n",
        "                .alias('realized_vol'),\n",
        "\n",
        "            # Parkinson volatility\n",
        "            (pl.col('high').log() - pl.col('low').log())\n",
        "                .rolling_std(window=20)\n",
        "                .mul(1/np.sqrt(4 * np.log(2)))\n",
        "                .alias('parkinson_vol'),\n",
        "\n",
        "            # GARCH volatility estimation\n",
        "            pl.col('returns').map_batches(self._estimate_garch)\n",
        "                .alias('garch_vol')\n",
        "        ]\n",
        "\n",
        "    def _market_regime_features(self) -> List[pl.Expr]:\n",
        "        \"\"\"Market regime identification features.\"\"\"\n",
        "        return [\n",
        "            # Trend strength\n",
        "            pl.col('close').map_batches(self._compute_hurst_exponent)\n",
        "                .alias('hurst_exponent'),\n",
        "\n",
        "            # Volatility regime\n",
        "            pl.col('realized_vol')\n",
        "                .map_batches(self._identify_vol_regime)\n",
        "                .alias('vol_regime'),\n",
        "\n",
        "            # Market efficiency ratio\n",
        "            (pl.col('close').diff().abs().rolling_sum(20) /\n",
        "             (pl.col('high').rolling_max(20) -\n",
        "              pl.col('low').rolling_min(20)))\n",
        "                .alias('efficiency_ratio')\n",
        "        ]\n",
        "\n",
        "    def _orderbook_features(self) -> List[pl.Expr]:\n",
        "        \"\"\"Order book derived features.\"\"\"\n",
        "        return [\n",
        "            # Bid-ask spread\n",
        "            ((pl.col('ask') - pl.col('bid')) / pl.col('mid'))\n",
        "                .alias('relative_spread'),\n",
        "\n",
        "            # Order book imbalance\n",
        "            ((pl.col('bid_size') - pl.col('ask_size')) /\n",
        "             (pl.col('bid_size') + pl.col('ask_size')))\n",
        "                .alias('ob_imbalance'),\n",
        "\n",
        "            # Market depth\n",
        "            (pl.col('bid_size') + pl.col('ask_size'))\n",
        "                .alias('market_depth')\n",
        "        ]\n",
        "\n",
        "    def _correlation_features(self) -> List[pl.Expr]:\n",
        "        \"\"\"Cross-asset correlation features.\"\"\"\n",
        "        return [\n",
        "            # Rolling correlation with market\n",
        "            pl.col('returns').rolling_corr(\n",
        "                pl.col('market_returns'),\n",
        "                window_size=20\n",
        "            ).alias('market_correlation'),\n",
        "\n",
        "            # Sector correlation\n",
        "            pl.col('returns').rolling_corr(\n",
        "                pl.col('sector_returns'),\n",
        "                window_size=20\n",
        "            ).alias('sector_correlation')\n",
        "        ]\n",
        "\n",
        "    @staticmethod\n",
        "    def _estimate_garch(returns: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Estimate GARCH(1,1) volatility.\"\"\"\n",
        "        # Implementation of GARCH estimation\n",
        "        omega = 0.000001\n",
        "        alpha = 0.1\n",
        "        beta = 0.8\n",
        "\n",
        "        vol = np.zeros_like(returns)\n",
        "        vol[0] = np.std(returns)\n",
        "\n",
        "        for t in range(1, len(returns)):\n",
        "            vol[t] = np.sqrt(\n",
        "                omega +\n",
        "                alpha * returns[t-1]**2 +\n",
        "                beta * vol[t-1]**2\n",
        "            )\n",
        "\n",
        "        return vol\n",
        "\n",
        "    @staticmethod\n",
        "    def _compute_hurst_exponent(prices: np.ndarray, lags: int = 20) -> float:\n",
        "        \"\"\"Compute Hurst exponent for trend strength.\"\"\"\n",
        "        # Implementation of Hurst exponent calculation\n",
        "        lags = range(2, lags)\n",
        "        tau = [np.sqrt(np.std(np.subtract(prices[lag:], prices[:-lag])))\n",
        "               for lag in lags]\n",
        "\n",
        "        reg = np.polyfit(np.log(lags), np.log(tau), 1)\n",
        "        return reg[0]  # Hurst exponent"
      ],
      "metadata": {
        "id": "XIccRiztgaxr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Integration with Ollama and LLMs\n",
        "\n",
        "We can enhance the system by incorporating large language models for sentiment analysis and market context:"
      ],
      "metadata": {
        "id": "4fFe7cS-goGj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from typing import List, Dict\n",
        "import aiohttp\n",
        "import json\n",
        "\n",
        "class MarketContextAnalyzer:\n",
        "    def __init__(self, config):\n",
        "        self.config = config\n",
        "        self.ollama_endpoint = \"http://localhost:11434/api/generate\"\n",
        "\n",
        "    async def analyze_market_context(\n",
        "        self,\n",
        "        market_data: Dict,\n",
        "        news_data: List[str],\n",
        "        social_data: List[str]\n",
        "    ) -> Dict:\n",
        "        \"\"\"Analyze market context using LLM.\"\"\"\n",
        "        prompt = self._construct_analysis_prompt(\n",
        "            market_data, news_data, social_data\n",
        "        )\n",
        "\n",
        "        async with aiohttp.ClientSession() as session:\n",
        "            async with session.post(\n",
        "                self.ollama_endpoint,\n",
        "                json={\n",
        "                    \"model\": \"llama2\",\n",
        "                    \"prompt\": prompt,\n",
        "                    \"stream\": False\n",
        "                }\n",
        "            ) as response:\n",
        "                result = await response.json()\n",
        "\n",
        "        return self._parse_llm_response(result['response'])\n",
        "\n",
        "    def _construct_analysis_prompt(\n",
        "        self,\n",
        "        market_data: Dict,\n",
        "        news_data: List[str],\n",
        "        social_data: List[str]\n",
        "    ) -> str:\n",
        "        \"\"\"Construct prompt for market analysis.\"\"\"\n",
        "        return f\"\"\"\n",
        "        Analyze the following market context:\n",
        "\n",
        "        Market Data:\n",
        "        - Current Price: {market_data['price']}\n",
        "        - 24h Change: {market_data['change_24h']}%\n",
        "        - Volume: {market_data['volume']}\n",
        "\n",
        "        Recent News:\n",
        "        {self._format_news(news_data)}\n",
        "\n",
        "        Social Sentiment:\n",
        "        {self._format_social(social_data)}\n",
        "\n",
        "        Provide analysis of:\n",
        "        1. Market sentiment\n",
        "        2. Key drivers\n",
        "        3. Risk factors\n",
        "        4. Trading opportunities\n",
        "\n",
        "        Format response as JSON with these keys.\n",
        "        \"\"\"\n",
        "\n",
        "    def _parse_llm_response(self, response: str) -> Dict:\n",
        "        \"\"\"Parse and validate LLM response.\"\"\"\n",
        "        try:\n",
        "            analysis = json.loads(response)\n",
        "            required_keys = [\n",
        "                'market_sentiment',\n",
        "                'key_drivers',"
      ],
      "metadata": {
        "id": "ILG9sIXdgpDx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generate Metadata Catalogue Indexing /CCXT"
      ],
      "metadata": {
        "id": "097vgOh9xSek"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------------------------------------------------\n",
        "# Generate metadata catalogue\n",
        "# --------------------------------------------------------------\n",
        "\n",
        "import os\n",
        "import sqlite3\n",
        "import pyarrow.parquet as pq\n",
        "import datetime\n",
        "from tqdm import tqdm\n",
        "from pathlib import Path\n",
        "import logging\n",
        "import yaml\n",
        "import time\n",
        "import shutil\n",
        "import json\n",
        "from concurrent.futures import ProcessPoolExecutor\n",
        "\n",
        "# Check if the script is running in Colab\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "except ModuleNotFoundError:\n",
        "    logging.info(\"Not running in Colab, skipping Google Drive mount.\")\n",
        "\n",
        "# Path to config file\n",
        "CONFIG_FILE = \"/content/drive/MyDrive/data_warehouse/config.yaml\"\n",
        "\n",
        "# Default configuration\n",
        "default_config = {\n",
        "    'batch_size': 1000,\n",
        "    'log_level': 'INFO',\n",
        "    'google_drive_dir': '/content/drive/MyDrive/data_warehouse/',\n",
        "    'db_file': '/content/drive/MyDrive/data_warehouse/parquet_metadata_catalog.db',\n",
        "    'backup_dir': '/content/drive/MyDrive/data_warehouse/backups/'\n",
        "}\n",
        "\n",
        "# Check if config.yaml exists, and if not, create it\n",
        "if not os.path.exists(CONFIG_FILE):\n",
        "    # Create the directory if it doesn't exist\n",
        "    os.makedirs(os.path.dirname(CONFIG_FILE), exist_ok=True)\n",
        "\n",
        "    # Write the default configuration to the file\n",
        "    with open(CONFIG_FILE, 'w') as yaml_file:\n",
        "        yaml.dump(default_config, yaml_file)\n",
        "    print(f\"Default config.yaml created at {CONFIG_FILE}\")\n",
        "\n",
        "# Load configuration from YAML\n",
        "with open(CONFIG_FILE, 'r') as f:\n",
        "    config = yaml.safe_load(f)\n",
        "\n",
        "# Configuration\n",
        "BATCH_SIZE = config.get('batch_size', 1000)\n",
        "LOG_LEVEL = config.get('log_level', 'INFO').upper()\n",
        "GOOGLE_DRIVE_DIR = Path(config.get('google_drive_dir', '/content/drive/MyDrive/data_warehouse/'))\n",
        "DB_FILE = Path(config.get('db_file', '/content/drive/MyDrive/data_warehouse/parquet_metadata_catalog.db'))\n",
        "BACKUP_DIR = Path(config.get('backup_dir', '/content/drive/MyDrive/data_warehouse/backups/'))\n",
        "\n",
        "# Setup logging\n",
        "logging.basicConfig(level=LOG_LEVEL, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "# Ensure directory exists for the DB and backup\n",
        "DB_FILE.parent.mkdir(parents=True, exist_ok=True)\n",
        "BACKUP_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Backup function\n",
        "def backup_database(db_file, backup_dir):\n",
        "    backup_file = backup_dir / f\"parquet_metadata_backup_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.db\"\n",
        "    shutil.copy(db_file, backup_file)\n",
        "    logging.info(f\"Database backup successful: {backup_file}\")\n",
        "\n",
        "# Function to extract metadata from Parquet files\n",
        "def extract_parquet_metadata(file_path):\n",
        "    try:\n",
        "        parquet_file = pq.ParquetFile(file_path)\n",
        "        source = file_path.stem  # Extract filename without extension\n",
        "        timeframe = file_path.parent.name  # Folder name as timeframe\n",
        "        schema_json = json.dumps({field.name: field.physical_type for field in parquet_file.schema})\n",
        "        return {\n",
        "            \"file_name\": file_path.name,\n",
        "            \"source\": source,\n",
        "            \"timeframe\": timeframe,\n",
        "            \"schema\": schema_json,\n",
        "            \"last_updated\": datetime.datetime.fromtimestamp(file_path.stat().st_mtime)\n",
        "        }\n",
        "    except pq.lib.ArrowInvalid as e:\n",
        "        logging.error(f\"Invalid Parquet file {file_path}: {e}\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error processing {file_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "# Initialize SQLite connection with timeout\n",
        "def initialize_db(db_file):\n",
        "    conn = sqlite3.connect(db_file, timeout=10)\n",
        "    cursor = conn.cursor()\n",
        "\n",
        "    # Create metadata and processed_files tables if they don't exist\n",
        "    cursor.execute('''\n",
        "        CREATE TABLE IF NOT EXISTS metadata (\n",
        "            id INTEGER PRIMARY KEY,\n",
        "            file_name TEXT,\n",
        "            source TEXT,\n",
        "            timeframe TEXT,\n",
        "            schema TEXT,\n",
        "            last_updated TIMESTAMP\n",
        "        )\n",
        "    ''')\n",
        "\n",
        "    cursor.execute('''\n",
        "        CREATE TABLE IF NOT EXISTS processed_files (\n",
        "            file_name TEXT PRIMARY KEY\n",
        "        )\n",
        "    ''')\n",
        "\n",
        "    # Create index to improve query performance\n",
        "    cursor.execute('CREATE INDEX IF NOT EXISTS idx_processed_file_name ON processed_files(file_name)')\n",
        "\n",
        "    conn.commit()\n",
        "    return conn, cursor\n",
        "\n",
        "# Process Parquet files concurrently and return the metadata\n",
        "def process_files_concurrently(files):\n",
        "    with ProcessPoolExecutor(max_workers=4) as executor:  # Adjust max_workers if necessary\n",
        "        metadata_list = list(tqdm(executor.map(extract_parquet_metadata, files), total=len(files), desc=\"Processing files\"))\n",
        "        return [m for m in metadata_list if m]  # Filter out None results\n",
        "\n",
        "# Main function to process Parquet files and insert metadata\n",
        "def process_parquet_files():\n",
        "    conn, cursor = initialize_db(DB_FILE)\n",
        "    metadata_list = []\n",
        "    failed_files = []\n",
        "\n",
        "    logging.info(\"Collecting Parquet files...\")\n",
        "    all_files = list(tqdm(GOOGLE_DRIVE_DIR.rglob(\"*.parquet\"), desc=\"Collecting Parquet files\"))\n",
        "    logging.info(f\"Found {len(all_files)} Parquet files to process.\")\n",
        "\n",
        "    try:\n",
        "        # Process files concurrently and collect metadata\n",
        "        metadata_list = process_files_concurrently(all_files)\n",
        "\n",
        "        # Batch insert metadata into the database\n",
        "        for i in range(0, len(metadata_list), BATCH_SIZE):\n",
        "            batch = metadata_list[i:i + BATCH_SIZE]\n",
        "            cursor.executemany('''\n",
        "                INSERT INTO metadata (file_name, source, timeframe, schema, last_updated)\n",
        "                VALUES (?, ?, ?, ?, ?)\n",
        "            ''', [(m['file_name'], m['source'], m['timeframe'], m['schema'], m['last_updated']) for m in batch])\n",
        "            conn.commit()\n",
        "            logging.info(f\"Inserted {len(batch)} records into the database.\")\n",
        "\n",
        "        # Mark files as processed\n",
        "        for metadata in metadata_list:\n",
        "            cursor.execute('INSERT OR IGNORE INTO processed_files (file_name) VALUES (?)', (metadata[\"file_name\"],))\n",
        "\n",
        "        # Retry logic for failed files (if any)\n",
        "        if failed_files:\n",
        "            logging.warning(f\"Retrying failed files: {failed_files}\")\n",
        "            for file in failed_files:\n",
        "                metadata = extract_parquet_metadata(GOOGLE_DRIVE_DIR / file)\n",
        "                if metadata:\n",
        "                    cursor.execute('''\n",
        "                        INSERT INTO metadata (file_name, source, timeframe, schema, last_updated)\n",
        "                        VALUES (?, ?, ?, ?, ?)\n",
        "                    ''', (\n",
        "                        metadata[\"file_name\"],\n",
        "                        metadata[\"source\"],\n",
        "                        metadata[\"timeframe\"],\n",
        "                        metadata[\"schema\"],\n",
        "                        metadata[\"last_updated\"]\n",
        "                    ))\n",
        "                    conn.commit()\n",
        "\n",
        "        # Create indexes for better query performance\n",
        "        logging.info(\"Creating indexes on source and timeframe columns...\")\n",
        "        cursor.execute('CREATE INDEX IF NOT EXISTS idx_source ON metadata(source)')\n",
        "        cursor.execute('CREATE INDEX IF NOT EXISTS idx_timeframe ON metadata(timeframe)')\n",
        "        conn.commit()\n",
        "\n",
        "        # Optimize the database\n",
        "        logging.info(\"Running VACUUM to optimize the database...\")\n",
        "        cursor.execute('VACUUM')\n",
        "\n",
        "    except KeyboardInterrupt:\n",
        "        logging.warning(\"Script interrupted by user.\")\n",
        "    finally:\n",
        "        # Close connection gracefully\n",
        "        conn.close()\n",
        "        logging.info(\"Database connection closed.\")\n",
        "        # Backup the database after processing\n",
        "        backup_database(DB_FILE, BACKUP_DIR)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Track total time for performance metrics\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Process Parquet files\n",
        "    process_parquet_files()\n",
        "\n",
        "    # Log total time\n",
        "    end_time = time.time()\n",
        "    logging.info(f\"Total processing time: {end_time - start_time} seconds\")\n"
      ],
      "metadata": {
        "id": "R676lCCc8cjF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train models"
      ],
      "metadata": {
        "id": "O2j7La_1DJK7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "Advanced Crypto Data Aggregator, Preprocessor, and Model Trainer using Polars\n",
        "with a Pre-Computed File Index for Fast Recursive Lookups\n",
        "\n",
        "This script performs the following tasks:\n",
        "1. Loads a metadata catalogue from a SQLite database.\n",
        "2. Recursively scans the ROOT_DIR once to build an index of all Parquet files.\n",
        "3. Uses that index to quickly locate files for each metadata record.\n",
        "4. Aggregates data from thousands of Parquet files (hundreds of exchanges/pairs).\n",
        "5. Applies advanced feature engineering (technical indicators, lag features,\n",
        "   one-hot encoding of categorical variables).\n",
        "6. Creates sequences for LSTM training.\n",
        "7. Splits data chronologically.\n",
        "8. Trains multiple models (baseline RandomForest/XGBoost with GridSearchCV tuning,\n",
        "   plus an LSTM model) and compares performance (MAE/MSE).\n",
        "9. Saves the best-performing model.\n",
        "10. Provides extensive logging and error handling.\n",
        "\n",
        "Dependencies:\n",
        "    - polars, numpy, pyarrow, tqdm\n",
        "    - scikit-learn, xgboost, tensorflow\n",
        "    - sqlite3 (built-in), pickle\n",
        "    - concurrent.futures\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import sqlite3\n",
        "import logging\n",
        "import json\n",
        "import datetime\n",
        "import time\n",
        "from pathlib import Path\n",
        "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
        "\n",
        "import polars as pl\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "# For model training (we switch to Pandas/NumPy as needed)\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential, load_model\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "\n",
        "import pickle\n",
        "\n",
        "# ------------------- Logging Configuration ------------------- #\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# ------------------- Global Configuration ------------------- #\n",
        "# Update these paths according to your Google Drive structure.\n",
        "DATA_FOLDER = Path(\"/content/drive/MyDrive/YourDataFolder\")  # (e.g., Folder with id: 1zk392ymU_pBtRgua-OWCq_wZyJ5ejnlF)\n",
        "ROOT_DIR = Path(\"/content/drive/MyDrive/data_warehouse\")       # (e.g., Folder with id: 1ACX5jdVuRwCUo_0L9rVUC3HOdGz06dPb)\n",
        "METADATA_DB = Path(\"/content/drive/MyDrive/data_warehouse/parquet_metadata_catalog.db\")  # (your metadata catalogue)\n",
        "\n",
        "# ------------------- 0. File Indexing ------------------- #\n",
        "def index_all_files(root_dir: Path) -> dict:\n",
        "    \"\"\"\n",
        "    Recursively index all .parquet files under root_dir.\n",
        "    Returns a dictionary mapping file names (e.g., \"binance_BTC_USDT_1h.parquet\")\n",
        "    to a list of full paths (Path objects). This is done only once to speed up lookups.\n",
        "    \"\"\"\n",
        "    logger.info(f\"Indexing all Parquet files under {root_dir} ...\")\n",
        "    file_index = {}\n",
        "    # os.walk is used for speed (uses os.scandir internally)\n",
        "    for dirpath, dirnames, filenames in os.walk(root_dir):\n",
        "        for fname in filenames:\n",
        "            if fname.endswith('.parquet'):\n",
        "                full_path = Path(dirpath) / fname\n",
        "                file_index.setdefault(fname, []).append(full_path)\n",
        "    logger.info(f\"Indexed {len(file_index)} unique Parquet file names.\")\n",
        "    return file_index\n",
        "\n",
        "# Global index variable (to be built once)\n",
        "FILE_INDEX = index_all_files(ROOT_DIR)\n",
        "\n",
        "def find_parquet_file(file_name: str, file_index: dict) -> Path:\n",
        "    \"\"\"\n",
        "    Look up a file by file_name using the pre-built file_index.\n",
        "    Returns the first matching Path if found; otherwise, logs a warning and returns None.\n",
        "    \"\"\"\n",
        "    if file_name in file_index:\n",
        "        return file_index[file_name][0]  # Return first occurrence\n",
        "    else:\n",
        "        logger.warning(f\"File {file_name} not found in index.\")\n",
        "        return None\n",
        "\n",
        "# ------------------- 1. Data Loader Functions (using Polars) ------------------- #\n",
        "def load_metadata(db_path: Path):\n",
        "    \"\"\"\n",
        "    Load metadata from the SQLite database.\n",
        "    Returns a pandas DataFrame containing metadata records.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        conn = sqlite3.connect(str(db_path))\n",
        "        import pandas as pd  # Use pandas here for SQL convenience\n",
        "        df_meta = pd.read_sql_query(\"SELECT * FROM metadata\", conn)\n",
        "        conn.close()\n",
        "        logger.info(f\"Loaded {len(df_meta)} metadata records.\")\n",
        "        return df_meta\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error loading metadata: {e}\")\n",
        "        sys.exit(1)\n",
        "\n",
        "def load_single_parquet_polars(file_path: Path) -> pl.DataFrame:\n",
        "    \"\"\"\n",
        "    Load a single Parquet file using Polars.\n",
        "    Extracts metadata (exchange, pair, timeframe) from the file name and folder structure,\n",
        "    and adds these as columns.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        df = pl.read_parquet(str(file_path))\n",
        "        file_stem = file_path.stem  # e.g., \"binance_BTC_USDT_1h\"\n",
        "        parts = file_stem.split('_')\n",
        "        if len(parts) >= 3:\n",
        "            exchange = parts[0]\n",
        "            pair = \"_\".join(parts[1:3])\n",
        "        else:\n",
        "            exchange = \"unknown\"\n",
        "            pair = \"unknown\"\n",
        "        timeframe = file_path.parent.name  # Use parent folder name as timeframe\n",
        "        df = df.with_columns([\n",
        "            pl.lit(exchange).alias(\"exchange\"),\n",
        "            pl.lit(pair).alias(\"pair\"),\n",
        "            pl.lit(timeframe).alias(\"timeframe\")\n",
        "        ])\n",
        "        return df\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error loading {file_path}: {e}\")\n",
        "        return pl.DataFrame()\n",
        "\n",
        "def aggregate_data(metadata_df, root_dir: Path, file_index: dict) -> pl.DataFrame:\n",
        "    \"\"\"\n",
        "    Aggregate all Parquet files listed in the metadata DataFrame.\n",
        "    Uses the pre-built file_index for fast lookups and ProcessPoolExecutor for parallel loading.\n",
        "    Returns a concatenated Polars DataFrame.\n",
        "    \"\"\"\n",
        "    file_paths = []\n",
        "    for _, row in metadata_df.iterrows():\n",
        "        file_name = row[\"file_name\"]\n",
        "        fp = find_parquet_file(file_name, file_index)\n",
        "        if fp is not None:\n",
        "            file_paths.append(fp)\n",
        "    logger.info(f\"Found {len(file_paths)} Parquet files to load (via index).\")\n",
        "\n",
        "    df_list = []\n",
        "    with ProcessPoolExecutor(max_workers=8) as executor:\n",
        "        futures = {executor.submit(load_single_parquet_polars, fp): fp for fp in file_paths}\n",
        "        for future in tqdm(as_completed(futures), total=len(futures), desc=\"Loading Parquet Files\"):\n",
        "            df_result = future.result()\n",
        "            if df_result.height > 0:\n",
        "                df_list.append(df_result)\n",
        "    if df_list:\n",
        "        aggregated_df = pl.concat(df_list)\n",
        "        if \"timestamp\" in aggregated_df.columns:\n",
        "            aggregated_df = aggregated_df.with_column(pl.col(\"timestamp\").cast(pl.Datetime))\n",
        "            aggregated_df = aggregated_df.drop_nulls(subset=[\"timestamp\"]).sort(\"timestamp\")\n",
        "        logger.info(f\"Aggregated DataFrame shape: {aggregated_df.shape}\")\n",
        "        return aggregated_df\n",
        "    else:\n",
        "        logger.error(\"No data loaded from Parquet files.\")\n",
        "        sys.exit(1)\n",
        "\n",
        "# ------------------- 2. Preprocessing Pipeline Functions (using Polars) ------------------- #\n",
        "def calculate_technical_indicators(df: pl.DataFrame) -> pl.DataFrame:\n",
        "    \"\"\"\n",
        "    Compute technical indicators on the 'close' price.\n",
        "    Computes approximate EMA_8, EMA_21 (using rolling means), SMA_10, and RSI_14.\n",
        "    \"\"\"\n",
        "    df = df.with_columns([\n",
        "        pl.col(\"close\").rolling_mean(window_size=8).alias(\"EMA_8\"),\n",
        "        pl.col(\"close\").rolling_mean(window_size=21).alias(\"EMA_21\"),\n",
        "        pl.col(\"close\").rolling_mean(window_size=10).alias(\"SMA_10\")\n",
        "    ])\n",
        "    # RSI calculation:\n",
        "    df = df.with_column(pl.col(\"close\").diff().alias(\"delta\"))\n",
        "    df = df.with_columns([\n",
        "        pl.when(pl.col(\"delta\") > 0).then(pl.col(\"delta\")).otherwise(0).alias(\"gain\"),\n",
        "        pl.when(pl.col(\"delta\") < 0).then(-pl.col(\"delta\")).otherwise(0).alias(\"loss\")\n",
        "    ])\n",
        "    df = df.with_columns([\n",
        "        pl.col(\"gain\").rolling_mean(window_size=14).alias(\"avg_gain\"),\n",
        "        pl.col(\"loss\").rolling_mean(window_size=14).alias(\"avg_loss\")\n",
        "    ])\n",
        "    df = df.with_columns([\n",
        "        (pl.col(\"avg_gain\") / pl.col(\"avg_loss\")).alias(\"rs\")\n",
        "    ])\n",
        "    df = df.with_columns([\n",
        "        (100 - (100 / (1 + pl.col(\"rs\")))).alias(\"RSI_14\")\n",
        "    ])\n",
        "    df = df.drop([\"delta\", \"gain\", \"loss\", \"avg_gain\", \"avg_loss\", \"rs\"])\n",
        "    return df\n",
        "\n",
        "def add_lag_features_and_target(df: pl.DataFrame, lag: int = 1) -> pl.DataFrame:\n",
        "    \"\"\"\n",
        "    Add lag features and define the target variable.\n",
        "    Target is defined as the next closing price.\n",
        "    \"\"\"\n",
        "    df = df.with_column(pl.col(\"close\").shift(lag).alias(\"close_lag_1\"))\n",
        "    df = df.with_column(pl.col(\"close\").shift(-1).alias(\"target\"))\n",
        "    df = df.drop_nulls()\n",
        "    return df\n",
        "\n",
        "def encode_categorical_features(df: pl.DataFrame) -> pl.DataFrame:\n",
        "    \"\"\"\n",
        "    One-hot encode categorical features: exchange, pair, timeframe.\n",
        "    Uses Polars' to_dummies.\n",
        "    \"\"\"\n",
        "    df = df.to_dummies(columns=[\"exchange\", \"pair\", \"timeframe\"])\n",
        "    return df\n",
        "\n",
        "def preprocess_data(df: pl.DataFrame) -> pl.DataFrame:\n",
        "    \"\"\"\n",
        "    Full preprocessing pipeline:\n",
        "      1. Calculate technical indicators.\n",
        "      2. Add lag features and target variable.\n",
        "      3. One-hot encode categorical features.\n",
        "    \"\"\"\n",
        "    logger.info(\"Calculating technical indicators...\")\n",
        "    df = calculate_technical_indicators(df)\n",
        "    logger.info(\"Adding lag features and target variable...\")\n",
        "    df = add_lag_features_and_target(df, lag=1)\n",
        "    logger.info(\"Encoding categorical features...\")\n",
        "    df = encode_categorical_features(df)\n",
        "    logger.info(f\"Preprocessed DataFrame shape: {df.shape}\")\n",
        "    return df\n",
        "\n",
        "def split_data(df: pl.DataFrame, test_ratio: float = 0.2):\n",
        "    \"\"\"\n",
        "    Chronologically split data into training and testing sets.\n",
        "    Returns X_train, X_test, y_train, y_test as Pandas objects (for ML compatibility).\n",
        "    \"\"\"\n",
        "    df = df.sort(\"timestamp\")\n",
        "    n_total = df.height\n",
        "    split_idx = int(n_total * (1 - test_ratio))\n",
        "    features = df.drop([\"timestamp\", \"target\"])\n",
        "    target = df.select(\"target\")\n",
        "    X = features.to_pandas()\n",
        "    y = target.to_pandas().values.ravel()\n",
        "    X_train = X.iloc[:split_idx]\n",
        "    X_test = X.iloc[split_idx:]\n",
        "    y_train = y[:split_idx]\n",
        "    y_test = y[split_idx:]\n",
        "    logger.info(f\"Split data: Training set: {X_train.shape}, Test set: {X_test.shape}\")\n",
        "    return X_train, X_test, y_train, y_test\n",
        "\n",
        "# ------------------- 3. Sequence Creation for LSTM ------------------- #\n",
        "def create_sequences(X, y, sequence_length: int = 10):\n",
        "    \"\"\"\n",
        "    Create sequences for LSTM training.\n",
        "    X: Pandas DataFrame, y: numpy array.\n",
        "    Returns: 3D numpy array for features and 1D array for targets.\n",
        "    \"\"\"\n",
        "    X_values = X.to_numpy()\n",
        "    y_values = y\n",
        "    X_seq, y_seq = [], []\n",
        "    for i in range(len(X_values) - sequence_length):\n",
        "        X_seq.append(X_values[i:i+sequence_length])\n",
        "        y_seq.append(y_values[i+sequence_length])\n",
        "    X_seq = np.array(X_seq)\n",
        "    y_seq = np.array(y_seq)\n",
        "    logger.info(f\"Created sequences: X_seq shape = {X_seq.shape}, y_seq shape = {y_seq.shape}\")\n",
        "    return X_seq, y_seq\n",
        "\n",
        "# ------------------- 4. Model Training Functions ------------------- #\n",
        "def train_rf_model(X_train, y_train, X_test, y_test):\n",
        "    \"\"\"\n",
        "    Train a RandomForestRegressor with hyperparameter tuning via GridSearchCV.\n",
        "    Returns the best model and its evaluation metrics.\n",
        "    \"\"\"\n",
        "    rf = RandomForestRegressor(random_state=42)\n",
        "    param_grid = {\"n_estimators\": [100, 200], \"max_depth\": [5, 10, None]}\n",
        "    grid = GridSearchCV(rf, param_grid, cv=3, scoring=\"neg_mean_absolute_error\", n_jobs=-1)\n",
        "    grid.fit(X_train, y_train)\n",
        "    best_rf = grid.best_estimator_\n",
        "    y_pred = best_rf.predict(X_test)\n",
        "    mae = mean_absolute_error(y_test, y_pred)\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    logger.info(f\"RandomForest best params: {grid.best_params_}\")\n",
        "    logger.info(f\"RandomForest - MAE: {mae:.4f}, MSE: {mse:.4f}\")\n",
        "    return best_rf, mae, mse\n",
        "\n",
        "def train_xgb_model(X_train, y_train, X_test, y_test):\n",
        "    \"\"\"\n",
        "    Train an XGBRegressor with hyperparameter tuning via GridSearchCV.\n",
        "    Returns the best model and its evaluation metrics.\n",
        "    \"\"\"\n",
        "    xgb = XGBRegressor(random_state=42)\n",
        "    param_grid = {\"n_estimators\": [100, 200], \"learning_rate\": [0.05, 0.1], \"max_depth\": [5, 10]}\n",
        "    grid = GridSearchCV(xgb, param_grid, cv=3, scoring=\"neg_mean_absolute_error\", n_jobs=-1)\n",
        "    grid.fit(X_train, y_train)\n",
        "    best_xgb = grid.best_estimator_\n",
        "    y_pred = best_xgb.predict(X_test)\n",
        "    mae = mean_absolute_error(y_test, y_pred)\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    logger.info(f\"XGBoost best params: {grid.best_params_}\")\n",
        "    logger.info(f\"XGBoost - MAE: {mae:.4f}, MSE: {mse:.4f}\")\n",
        "    return best_xgb, mae, mse\n",
        "\n",
        "def train_lstm_model(X_train, y_train, X_test, y_test, epochs=20, batch_size=32, sequence_length=10):\n",
        "    \"\"\"\n",
        "    Train an LSTM model on sequential data.\n",
        "    Returns the trained LSTM model and its evaluation metrics.\n",
        "    \"\"\"\n",
        "    X_train_seq, y_train_seq = create_sequences(X_train, y_train, sequence_length)\n",
        "    X_test_seq, y_test_seq = create_sequences(X_test, y_test, sequence_length)\n",
        "\n",
        "    model = Sequential([\n",
        "        LSTM(64, return_sequences=True, input_shape=(X_train_seq.shape[1], X_train_seq.shape[2])),\n",
        "        Dropout(0.2),\n",
        "        LSTM(64),\n",
        "        Dropout(0.2),\n",
        "        Dense(1)\n",
        "    ])\n",
        "    model.compile(optimizer=\"adam\", loss=\"mean_squared_error\")\n",
        "    model.fit(X_train_seq, y_train_seq, epochs=epochs, batch_size=batch_size,\n",
        "              validation_data=(X_test_seq, y_test_seq), verbose=1)\n",
        "\n",
        "    y_pred_seq = model.predict(X_test_seq).flatten()\n",
        "    mae = mean_absolute_error(y_test_seq, y_pred_seq)\n",
        "    mse = mean_squared_error(y_test_seq, y_pred_seq)\n",
        "    logger.info(f\"LSTM - MAE: {mae:.4f}, MSE: {mse:.4f}\")\n",
        "    return model, mae, mse\n",
        "\n",
        "def compare_model_results(results_dict):\n",
        "    \"\"\"\n",
        "    Compare models based on MAE, log performance, and return the key of the best model.\n",
        "    \"\"\"\n",
        "    best_model_key = None\n",
        "    best_mae = float(\"inf\")\n",
        "    for model_name, metrics in results_dict.items():\n",
        "        mae = metrics[\"MAE\"]\n",
        "        mse = metrics[\"MSE\"]\n",
        "        logger.info(f\"Model {model_name}: MAE = {mae:.4f}, MSE = {mse:.4f}\")\n",
        "        if mae < best_mae:\n",
        "            best_mae = mae\n",
        "            best_model_key = model_name\n",
        "    logger.info(f\"Best model: {best_model_key} with MAE {best_mae:.4f}\")\n",
        "    return best_model_key\n",
        "\n",
        "# ------------------- 5. Orchestration: Main Execution ------------------- #\n",
        "def main():\n",
        "    overall_start = time.time()\n",
        "    # Step 1: Load metadata from SQLite\n",
        "    metadata_df = load_metadata(METADATA_DB)\n",
        "\n",
        "    # Step 2: Aggregate Parquet data using the pre-built FILE_INDEX\n",
        "    aggregated_df = aggregate_data(metadata_df, ROOT_DIR, FILE_INDEX)\n",
        "\n",
        "    # Step 3: Preprocess the aggregated data using Polars\n",
        "    preprocessed_df = preprocess_data(aggregated_df)\n",
        "\n",
        "    # Step 4: Split the data chronologically into training and testing sets\n",
        "    X_train, X_test, y_train, y_test = split_data(preprocessed_df, test_ratio=0.2)\n",
        "\n",
        "    # Step 5: Train models\n",
        "    results = {}\n",
        "    rf_model, rf_mae, rf_mse = train_rf_model(X_train, y_train, X_test, y_test)\n",
        "    results[\"RandomForest\"] = {\"model\": rf_model, \"MAE\": rf_mae, \"MSE\": rf_mse}\n",
        "\n",
        "    xgb_model, xgb_mae, xgb_mse = train_xgb_model(X_train, y_train, X_test, y_test)\n",
        "    results[\"XGBoost\"] = {\"model\": xgb_model, \"MAE\": xgb_mae, \"MSE\": xgb_mse}\n",
        "\n",
        "    lstm_model, lstm_mae, lstm_mse = train_lstm_model(X_train, y_train, X_test, y_test,\n",
        "                                                       epochs=20, batch_size=32, sequence_length=10)\n",
        "    results[\"LSTM\"] = {\"model\": lstm_model, \"MAE\": lstm_mae, \"MSE\": lstm_mse}\n",
        "\n",
        "    # Step 6: Compare models and select the best one based on MAE\n",
        "    best_model_key = compare_model_results(results)\n",
        "\n",
        "    # Step 7: Save the best-performing model\n",
        "    if best_model_key == \"LSTM\":\n",
        "        save_path = \"best_lstm_model.h5\"\n",
        "        results[\"LSTM\"][\"model\"].save(save_path)\n",
        "    else:\n",
        "        save_path = f\"best_{best_model_key}_model.pkl\"\n",
        "        with open(save_path, \"wb\") as f:\n",
        "            pickle.dump(results[best_model_key][\"model\"], f)\n",
        "    logger.info(f\"Best model saved to {save_path}\")\n",
        "\n",
        "    overall_end = time.time()\n",
        "    logger.info(f\"Total execution time: {overall_end - overall_start:.2f} seconds\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "GJv2n-7YDI-J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "100 years  later"
      ],
      "metadata": {
        "id": "mPVMQldeL01t"
      }
    }
  ]
}